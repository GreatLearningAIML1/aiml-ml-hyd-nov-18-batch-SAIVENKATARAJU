{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R8_External_Lab_Questions-HYD_AIML_Nov18.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QGIsF1ADyJ58"
      },
      "source": [
        "# Transfer Learning CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E-n6tVFayGBe"
      },
      "source": [
        "* Train a simple convnet on the CIFAR dataset the first 5 output classes [0..4].\n",
        "* Freeze convolutional layers and fine-tune dense layers for the last 5 ouput classes [5..9].\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Cq8ejXHJyGYq"
      },
      "source": [
        "### 1. Import CIFAR10 data and create 2 datasets with one dataset having classes from 0 to 4 and other having classes from 5 to 9 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gybWieoeiWq0",
        "colab_type": "code",
        "outputId": "6e889dfd-69f6-4ceb-e9dd-4ab524bc0aa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D,MaxPool2D,Flatten,Dropout\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uWYbxnBayFUP",
        "outputId": "cb2fd17f-64a9-4a23-8c0d-ec28e65cb1eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "(xtrain,ytrain),(xtest,ytest)=cifar10.load_data()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 7s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq0D5O1ij32N",
        "colab_type": "code",
        "outputId": "836febce-b53a-45bd-d142-a3b5b3cc886e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "xtrain.shape,ytrain.shape,xtest.shape,ytest.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 32, 32, 3), (50000, 1), (10000, 32, 32, 3), (10000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2gCUZjzk5VJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ytrain=ytrain.flatten()\n",
        "ytest=ytest.flatten()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1O9IzCYiy7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_lt5 = xtrain[ytrain < 5]\n",
        "y_train_lt5 = ytrain[ytrain < 5]\n",
        "x_test_lt5 = xtest[ytest < 5]\n",
        "y_test_lt5 = ytest[ytest < 5]\n",
        "\n",
        "x_train_gt5 = xtrain[ytrain >= 5]\n",
        "y_train_gt5 = ytrain[ytrain >= 5] - 5  # make classes start at 0 for\n",
        "x_test_gt5 = xtest[ytest >= 5]         # np_utils.to_categorical\n",
        "y_test_gt5 = ytest[ytest >= 5] - 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gvWwARhjwVV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xtCKmQh4yXhT"
      },
      "source": [
        "### 2. Use One-hot encoding to divide y_train and y_test into required no of output classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uN5O2kJ3yYa6",
        "colab": {}
      },
      "source": [
        "y_train_lt5=to_categorical(y_train_lt5,num_classes=5)\n",
        "y_test_lt5=to_categorical(y_test_lt5,num_classes=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6shWI5SjUuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_gt5=to_categorical(y_train_gt5,num_classes=5)\n",
        "y_test_gt5=to_categorical(y_test_gt5,num_classes=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEl0iSWUoldX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_lt5=(x_train_lt5/255).astype('float32')\n",
        "x_test_lt5=(x_test_lt5/255).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkj9nn4fjqDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_gt5=(x_train_gt5/255).astype('float32')\n",
        "x_test_gt5=(x_test_gt5/255).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cuOiKWfeybAl"
      },
      "source": [
        "### 3. Build a sequential neural network model which can classify the classes 0 to 4 of CIFAR10 dataset with at least 80% accuracy on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5HzxNbiiyoBD",
        "outputId": "22699bde-ee5b-4aa8-f7b7-54fb8f138f61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872
        }
      },
      "source": [
        "model =Sequential()\n",
        "model.add(Conv2D(64,(3,3),input_shape=(32,32,3),activation='relu'))\n",
        "model.add(Conv2D(32,(3,3),activation='relu'))\n",
        "model.add(MaxPool2D((2,2)))\n",
        "model.add(Dropout(0.20))\n",
        "\n",
        "model.add(Conv2D(64,(3,3),activation='relu'))\n",
        "model.add(Conv2D(32,(3,3),activation='relu'))\n",
        "model.add(MaxPool2D((2,2)))\n",
        "model.add(Dropout(0.20))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128,activation='relu'))\n",
        "\n",
        "model.add(Dense(5,activation='softmax'))\n",
        "model.compile('adam',metrics=['acc'],loss='categorical_crossentropy')\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0710 15:03:48.425228 140121292216192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0710 15:03:48.462348 140121292216192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0710 15:03:48.470194 140121292216192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0710 15:03:48.506686 140121292216192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0710 15:03:48.510549 140121292216192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0710 15:03:48.520270 140121292216192 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0710 15:03:48.604927 140121292216192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0710 15:03:48.625816 140121292216192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 30, 30, 64)        1792      \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 32)        18464     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 12, 12, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 10, 10, 32)        18464     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               102528    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 160,389\n",
            "Trainable params: 160,389\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nCPGlgRo81g",
        "colab_type": "code",
        "outputId": "42e96328-816c-4b6d-dc2f-605c16e643ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x_train_lt5,y_train_lt5,batch_size=1500,epochs=50,validation_data=(x_test_lt5,y_test_lt5))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0710 15:03:50.737560 140121292216192 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 5000 samples\n",
            "Epoch 1/50\n",
            "25000/25000 [==============================] - 11s 428us/step - loss: 1.4439 - acc: 0.3703 - val_loss: 1.2232 - val_acc: 0.4974\n",
            "Epoch 2/50\n",
            "25000/25000 [==============================] - 2s 78us/step - loss: 1.1910 - acc: 0.5111 - val_loss: 1.1002 - val_acc: 0.5562\n",
            "Epoch 3/50\n",
            "25000/25000 [==============================] - 2s 78us/step - loss: 1.1017 - acc: 0.5448 - val_loss: 1.0403 - val_acc: 0.5730\n",
            "Epoch 4/50\n",
            "25000/25000 [==============================] - 2s 78us/step - loss: 1.0366 - acc: 0.5708 - val_loss: 0.9914 - val_acc: 0.5860\n",
            "Epoch 5/50\n",
            "25000/25000 [==============================] - 2s 79us/step - loss: 0.9997 - acc: 0.5893 - val_loss: 0.9638 - val_acc: 0.6050\n",
            "Epoch 6/50\n",
            "25000/25000 [==============================] - 2s 79us/step - loss: 0.9655 - acc: 0.6088 - val_loss: 0.9111 - val_acc: 0.6302\n",
            "Epoch 7/50\n",
            "25000/25000 [==============================] - 2s 79us/step - loss: 0.9321 - acc: 0.6202 - val_loss: 0.9070 - val_acc: 0.6264\n",
            "Epoch 8/50\n",
            "25000/25000 [==============================] - 2s 79us/step - loss: 0.9035 - acc: 0.6341 - val_loss: 0.8722 - val_acc: 0.6454\n",
            "Epoch 9/50\n",
            "25000/25000 [==============================] - 2s 79us/step - loss: 0.8880 - acc: 0.6426 - val_loss: 0.8535 - val_acc: 0.6608\n",
            "Epoch 10/50\n",
            "25000/25000 [==============================] - 2s 79us/step - loss: 0.8684 - acc: 0.6475 - val_loss: 0.8465 - val_acc: 0.6560\n",
            "Epoch 11/50\n",
            "25000/25000 [==============================] - 2s 79us/step - loss: 0.8567 - acc: 0.6585 - val_loss: 0.8366 - val_acc: 0.6644\n",
            "Epoch 12/50\n",
            "25000/25000 [==============================] - 2s 79us/step - loss: 0.8373 - acc: 0.6693 - val_loss: 0.8275 - val_acc: 0.6708\n",
            "Epoch 13/50\n",
            "25000/25000 [==============================] - 2s 79us/step - loss: 0.8198 - acc: 0.6764 - val_loss: 0.8035 - val_acc: 0.6786\n",
            "Epoch 14/50\n",
            "25000/25000 [==============================] - 2s 80us/step - loss: 0.8108 - acc: 0.6849 - val_loss: 0.8251 - val_acc: 0.6810\n",
            "Epoch 15/50\n",
            "25000/25000 [==============================] - 2s 79us/step - loss: 0.7971 - acc: 0.6859 - val_loss: 0.7779 - val_acc: 0.6946\n",
            "Epoch 16/50\n",
            "25000/25000 [==============================] - 2s 80us/step - loss: 0.7912 - acc: 0.6904 - val_loss: 0.7820 - val_acc: 0.6934\n",
            "Epoch 17/50\n",
            "25000/25000 [==============================] - 2s 80us/step - loss: 0.7703 - acc: 0.7011 - val_loss: 0.7466 - val_acc: 0.7082\n",
            "Epoch 18/50\n",
            "25000/25000 [==============================] - 2s 80us/step - loss: 0.7528 - acc: 0.7090 - val_loss: 0.7304 - val_acc: 0.7152\n",
            "Epoch 19/50\n",
            "25000/25000 [==============================] - 2s 80us/step - loss: 0.7371 - acc: 0.7136 - val_loss: 0.7240 - val_acc: 0.7212\n",
            "Epoch 20/50\n",
            "25000/25000 [==============================] - 2s 80us/step - loss: 0.7296 - acc: 0.7193 - val_loss: 0.7599 - val_acc: 0.7032\n",
            "Epoch 21/50\n",
            "25000/25000 [==============================] - 2s 81us/step - loss: 0.7300 - acc: 0.7168 - val_loss: 0.7150 - val_acc: 0.7230\n",
            "Epoch 22/50\n",
            "25000/25000 [==============================] - 2s 80us/step - loss: 0.7221 - acc: 0.7221 - val_loss: 0.6871 - val_acc: 0.7392\n",
            "Epoch 23/50\n",
            "25000/25000 [==============================] - 2s 80us/step - loss: 0.6934 - acc: 0.7335 - val_loss: 0.6726 - val_acc: 0.7438\n",
            "Epoch 24/50\n",
            "25000/25000 [==============================] - 2s 79us/step - loss: 0.6809 - acc: 0.7386 - val_loss: 0.6783 - val_acc: 0.7450\n",
            "Epoch 25/50\n",
            "25000/25000 [==============================] - 2s 79us/step - loss: 0.6779 - acc: 0.7399 - val_loss: 0.6869 - val_acc: 0.7452\n",
            "Epoch 26/50\n",
            "25000/25000 [==============================] - 2s 79us/step - loss: 0.6681 - acc: 0.7466 - val_loss: 0.6465 - val_acc: 0.7522\n",
            "Epoch 27/50\n",
            "25000/25000 [==============================] - 2s 79us/step - loss: 0.6560 - acc: 0.7494 - val_loss: 0.6293 - val_acc: 0.7542\n",
            "Epoch 28/50\n",
            "25000/25000 [==============================] - 2s 79us/step - loss: 0.6411 - acc: 0.7538 - val_loss: 0.6396 - val_acc: 0.7532\n",
            "Epoch 29/50\n",
            "25000/25000 [==============================] - 2s 78us/step - loss: 0.6279 - acc: 0.7611 - val_loss: 0.6233 - val_acc: 0.7596\n",
            "Epoch 30/50\n",
            "25000/25000 [==============================] - 2s 79us/step - loss: 0.6192 - acc: 0.7651 - val_loss: 0.6106 - val_acc: 0.7664\n",
            "Epoch 31/50\n",
            "25000/25000 [==============================] - 2s 78us/step - loss: 0.6087 - acc: 0.7690 - val_loss: 0.6022 - val_acc: 0.7728\n",
            "Epoch 32/50\n",
            "25000/25000 [==============================] - 2s 78us/step - loss: 0.5944 - acc: 0.7766 - val_loss: 0.6070 - val_acc: 0.7748\n",
            "Epoch 33/50\n",
            "25000/25000 [==============================] - 2s 78us/step - loss: 0.5927 - acc: 0.7760 - val_loss: 0.5937 - val_acc: 0.7766\n",
            "Epoch 34/50\n",
            "25000/25000 [==============================] - 2s 78us/step - loss: 0.5767 - acc: 0.7822 - val_loss: 0.5974 - val_acc: 0.7746\n",
            "Epoch 35/50\n",
            "25000/25000 [==============================] - 2s 78us/step - loss: 0.5663 - acc: 0.7857 - val_loss: 0.5908 - val_acc: 0.7774\n",
            "Epoch 36/50\n",
            "25000/25000 [==============================] - 2s 79us/step - loss: 0.5669 - acc: 0.7836 - val_loss: 0.5775 - val_acc: 0.7840\n",
            "Epoch 37/50\n",
            "25000/25000 [==============================] - 2s 78us/step - loss: 0.5577 - acc: 0.7896 - val_loss: 0.5691 - val_acc: 0.7896\n",
            "Epoch 38/50\n",
            "25000/25000 [==============================] - 2s 79us/step - loss: 0.5436 - acc: 0.7944 - val_loss: 0.5843 - val_acc: 0.7864\n",
            "Epoch 39/50\n",
            "25000/25000 [==============================] - 2s 78us/step - loss: 0.5409 - acc: 0.7974 - val_loss: 0.5694 - val_acc: 0.7862\n",
            "Epoch 40/50\n",
            "25000/25000 [==============================] - 2s 79us/step - loss: 0.5288 - acc: 0.7992 - val_loss: 0.5822 - val_acc: 0.7818\n",
            "Epoch 41/50\n",
            "25000/25000 [==============================] - 2s 78us/step - loss: 0.5208 - acc: 0.8038 - val_loss: 0.5571 - val_acc: 0.7906\n",
            "Epoch 42/50\n",
            "25000/25000 [==============================] - 2s 78us/step - loss: 0.5163 - acc: 0.8066 - val_loss: 0.5477 - val_acc: 0.7978\n",
            "Epoch 43/50\n",
            "25000/25000 [==============================] - 2s 79us/step - loss: 0.5168 - acc: 0.8039 - val_loss: 0.5520 - val_acc: 0.7934\n",
            "Epoch 44/50\n",
            "25000/25000 [==============================] - 2s 79us/step - loss: 0.5139 - acc: 0.8056 - val_loss: 0.5588 - val_acc: 0.7978\n",
            "Epoch 45/50\n",
            "25000/25000 [==============================] - 2s 78us/step - loss: 0.5002 - acc: 0.8118 - val_loss: 0.5360 - val_acc: 0.8004\n",
            "Epoch 46/50\n",
            "25000/25000 [==============================] - 2s 77us/step - loss: 0.4998 - acc: 0.8103 - val_loss: 0.5448 - val_acc: 0.7970\n",
            "Epoch 47/50\n",
            "25000/25000 [==============================] - 2s 78us/step - loss: 0.4869 - acc: 0.8184 - val_loss: 0.5304 - val_acc: 0.8054\n",
            "Epoch 48/50\n",
            "25000/25000 [==============================] - 2s 78us/step - loss: 0.4762 - acc: 0.8207 - val_loss: 0.5245 - val_acc: 0.8070\n",
            "Epoch 49/50\n",
            "25000/25000 [==============================] - 2s 78us/step - loss: 0.4689 - acc: 0.8244 - val_loss: 0.5441 - val_acc: 0.7998\n",
            "Epoch 50/50\n",
            "25000/25000 [==============================] - 2s 78us/step - loss: 0.4732 - acc: 0.8241 - val_loss: 0.5478 - val_acc: 0.7998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f703e80be48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "woTfNst_ynRG"
      },
      "source": [
        "### 4. In the model which was built above (for classification of classes 0-4 in CIFAR10), make only the dense layers to be trainable and conv layers to be non-trainable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o_VCDB3Byb1a",
        "outputId": "4402ca67-f788-47e2-be97-5a9ba454228c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        }
      },
      "source": [
        "for layers in model.layers:\n",
        "    print(layers.name)\n",
        "    if('dense' not in layers.name):\n",
        "        layers.trainable = False\n",
        "        print(layers.name + 'is not trainable\\n')\n",
        "    if('dense' in layers.name):\n",
        "        print(layers.name + ' is trainable\\n')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv2d_1\n",
            "conv2d_1is not trainable\n",
            "\n",
            "conv2d_2\n",
            "conv2d_2is not trainable\n",
            "\n",
            "max_pooling2d_1\n",
            "max_pooling2d_1is not trainable\n",
            "\n",
            "dropout_1\n",
            "dropout_1is not trainable\n",
            "\n",
            "conv2d_3\n",
            "conv2d_3is not trainable\n",
            "\n",
            "conv2d_4\n",
            "conv2d_4is not trainable\n",
            "\n",
            "max_pooling2d_2\n",
            "max_pooling2d_2is not trainable\n",
            "\n",
            "dropout_2\n",
            "dropout_2is not trainable\n",
            "\n",
            "flatten_1\n",
            "flatten_1is not trainable\n",
            "\n",
            "dense_1\n",
            "dense_1 is trainable\n",
            "\n",
            "dense_2\n",
            "dense_2 is trainable\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1-uUPqWpyeyX"
      },
      "source": [
        "### 5. Utilize the the model trained on CIFAR 10 (classes 0 to 4) to classify the classes 5 to 9 of CIFAR 10  (Use Transfer Learning) <br>\n",
        "Achieve an accuracy of more than 85% on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "szHjJgDvyfCt",
        "outputId": "e11f84b8-fb57-4f35-c59d-8a96bae6013a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x_train_gt5,y_train_gt5,validation_data=(x_test_gt5,y_test_gt5),batch_size=32,epochs=50)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 5000 samples\n",
            "Epoch 1/50\n",
            "  480/25000 [..............................] - ETA: 9s - loss: 2.1287 - acc: 0.2125 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 6s 228us/step - loss: 1.0945 - acc: 0.5521 - val_loss: 0.7488 - val_acc: 0.7084\n",
            "Epoch 2/50\n",
            "25000/25000 [==============================] - 5s 215us/step - loss: 0.6846 - acc: 0.7400 - val_loss: 0.6554 - val_acc: 0.7544\n",
            "Epoch 3/50\n",
            "25000/25000 [==============================] - 5s 214us/step - loss: 0.5579 - acc: 0.7920 - val_loss: 0.4778 - val_acc: 0.8224\n",
            "Epoch 4/50\n",
            "25000/25000 [==============================] - 5s 212us/step - loss: 0.4781 - acc: 0.8258 - val_loss: 0.4893 - val_acc: 0.8224\n",
            "Epoch 5/50\n",
            "25000/25000 [==============================] - 5s 213us/step - loss: 0.4201 - acc: 0.8448 - val_loss: 0.3948 - val_acc: 0.8590\n",
            "Epoch 6/50\n",
            "25000/25000 [==============================] - 5s 217us/step - loss: 0.3785 - acc: 0.8612 - val_loss: 0.4009 - val_acc: 0.8570\n",
            "Epoch 7/50\n",
            "25000/25000 [==============================] - 5s 215us/step - loss: 0.3494 - acc: 0.8732 - val_loss: 0.3621 - val_acc: 0.8690\n",
            "Epoch 8/50\n",
            "25000/25000 [==============================] - 5s 215us/step - loss: 0.3158 - acc: 0.8853 - val_loss: 0.3329 - val_acc: 0.8862\n",
            "Epoch 9/50\n",
            "25000/25000 [==============================] - 5s 215us/step - loss: 0.2918 - acc: 0.8933 - val_loss: 0.3243 - val_acc: 0.8848\n",
            "Epoch 10/50\n",
            "25000/25000 [==============================] - 5s 218us/step - loss: 0.2692 - acc: 0.9027 - val_loss: 0.3352 - val_acc: 0.8858\n",
            "Epoch 11/50\n",
            "25000/25000 [==============================] - 5s 214us/step - loss: 0.2539 - acc: 0.9074 - val_loss: 0.3321 - val_acc: 0.8816\n",
            "Epoch 12/50\n",
            "25000/25000 [==============================] - 5s 214us/step - loss: 0.2348 - acc: 0.9148 - val_loss: 0.3915 - val_acc: 0.8682\n",
            "Epoch 13/50\n",
            "25000/25000 [==============================] - 5s 213us/step - loss: 0.2251 - acc: 0.9171 - val_loss: 0.3142 - val_acc: 0.8880\n",
            "Epoch 14/50\n",
            "25000/25000 [==============================] - 5s 213us/step - loss: 0.2079 - acc: 0.9236 - val_loss: 0.3006 - val_acc: 0.8996\n",
            "Epoch 15/50\n",
            "25000/25000 [==============================] - 5s 213us/step - loss: 0.1949 - acc: 0.9290 - val_loss: 0.3278 - val_acc: 0.8928\n",
            "Epoch 16/50\n",
            "25000/25000 [==============================] - 5s 212us/step - loss: 0.1906 - acc: 0.9304 - val_loss: 0.3564 - val_acc: 0.8830\n",
            "Epoch 17/50\n",
            "25000/25000 [==============================] - 5s 213us/step - loss: 0.1696 - acc: 0.9394 - val_loss: 0.3249 - val_acc: 0.8916\n",
            "Epoch 18/50\n",
            "25000/25000 [==============================] - 5s 211us/step - loss: 0.1701 - acc: 0.9386 - val_loss: 0.3325 - val_acc: 0.8938\n",
            "Epoch 19/50\n",
            "25000/25000 [==============================] - 5s 213us/step - loss: 0.1566 - acc: 0.9432 - val_loss: 0.3327 - val_acc: 0.8914\n",
            "Epoch 20/50\n",
            "25000/25000 [==============================] - 5s 211us/step - loss: 0.1502 - acc: 0.9449 - val_loss: 0.3843 - val_acc: 0.8782\n",
            "Epoch 21/50\n",
            "25000/25000 [==============================] - 5s 207us/step - loss: 0.1433 - acc: 0.9486 - val_loss: 0.3397 - val_acc: 0.8976\n",
            "Epoch 22/50\n",
            "25000/25000 [==============================] - 5s 213us/step - loss: 0.1359 - acc: 0.9493 - val_loss: 0.3939 - val_acc: 0.8812\n",
            "Epoch 23/50\n",
            "25000/25000 [==============================] - 5s 214us/step - loss: 0.1330 - acc: 0.9522 - val_loss: 0.3442 - val_acc: 0.8982\n",
            "Epoch 24/50\n",
            "25000/25000 [==============================] - 5s 216us/step - loss: 0.1351 - acc: 0.9516 - val_loss: 0.3484 - val_acc: 0.8964\n",
            "Epoch 25/50\n",
            "25000/25000 [==============================] - 5s 216us/step - loss: 0.1253 - acc: 0.9533 - val_loss: 0.3521 - val_acc: 0.8934\n",
            "Epoch 26/50\n",
            "25000/25000 [==============================] - 5s 215us/step - loss: 0.1180 - acc: 0.9583 - val_loss: 0.3507 - val_acc: 0.8954\n",
            "Epoch 27/50\n",
            "25000/25000 [==============================] - 5s 213us/step - loss: 0.1140 - acc: 0.9584 - val_loss: 0.3856 - val_acc: 0.8934\n",
            "Epoch 28/50\n",
            "25000/25000 [==============================] - 5s 214us/step - loss: 0.1198 - acc: 0.9567 - val_loss: 0.3737 - val_acc: 0.8952\n",
            "Epoch 29/50\n",
            "25000/25000 [==============================] - 5s 215us/step - loss: 0.1076 - acc: 0.9620 - val_loss: 0.4109 - val_acc: 0.8944\n",
            "Epoch 30/50\n",
            "25000/25000 [==============================] - 5s 215us/step - loss: 0.1055 - acc: 0.9612 - val_loss: 0.3904 - val_acc: 0.8978\n",
            "Epoch 31/50\n",
            "25000/25000 [==============================] - 5s 213us/step - loss: 0.1101 - acc: 0.9605 - val_loss: 0.3551 - val_acc: 0.8968\n",
            "Epoch 32/50\n",
            "25000/25000 [==============================] - 5s 214us/step - loss: 0.1002 - acc: 0.9654 - val_loss: 0.4013 - val_acc: 0.8894\n",
            "Epoch 33/50\n",
            "25000/25000 [==============================] - 5s 214us/step - loss: 0.1023 - acc: 0.9648 - val_loss: 0.4311 - val_acc: 0.8932\n",
            "Epoch 34/50\n",
            "25000/25000 [==============================] - 5s 212us/step - loss: 0.0957 - acc: 0.9660 - val_loss: 0.3767 - val_acc: 0.8994\n",
            "Epoch 35/50\n",
            "25000/25000 [==============================] - 5s 212us/step - loss: 0.0976 - acc: 0.9660 - val_loss: 0.3983 - val_acc: 0.8922\n",
            "Epoch 36/50\n",
            "25000/25000 [==============================] - 5s 214us/step - loss: 0.0894 - acc: 0.9685 - val_loss: 0.4253 - val_acc: 0.8944\n",
            "Epoch 37/50\n",
            "25000/25000 [==============================] - 5s 215us/step - loss: 0.1005 - acc: 0.9652 - val_loss: 0.4292 - val_acc: 0.8842\n",
            "Epoch 38/50\n",
            "25000/25000 [==============================] - 5s 215us/step - loss: 0.0925 - acc: 0.9664 - val_loss: 0.4108 - val_acc: 0.8958\n",
            "Epoch 39/50\n",
            "25000/25000 [==============================] - 5s 216us/step - loss: 0.0925 - acc: 0.9680 - val_loss: 0.3793 - val_acc: 0.8980\n",
            "Epoch 40/50\n",
            "25000/25000 [==============================] - 5s 217us/step - loss: 0.0851 - acc: 0.9692 - val_loss: 0.4719 - val_acc: 0.8790\n",
            "Epoch 41/50\n",
            "25000/25000 [==============================] - 5s 215us/step - loss: 0.0943 - acc: 0.9667 - val_loss: 0.3969 - val_acc: 0.8956\n",
            "Epoch 42/50\n",
            "25000/25000 [==============================] - 5s 215us/step - loss: 0.0857 - acc: 0.9699 - val_loss: 0.4061 - val_acc: 0.8978\n",
            "Epoch 43/50\n",
            "25000/25000 [==============================] - 5s 216us/step - loss: 0.0827 - acc: 0.9712 - val_loss: 0.4149 - val_acc: 0.8966\n",
            "Epoch 44/50\n",
            "25000/25000 [==============================] - 6s 222us/step - loss: 0.0911 - acc: 0.9693 - val_loss: 0.4070 - val_acc: 0.8964\n",
            "Epoch 45/50\n",
            "25000/25000 [==============================] - 5s 219us/step - loss: 0.0793 - acc: 0.9723 - val_loss: 0.4271 - val_acc: 0.8990\n",
            "Epoch 46/50\n",
            "25000/25000 [==============================] - 5s 215us/step - loss: 0.0782 - acc: 0.9723 - val_loss: 0.4530 - val_acc: 0.8940\n",
            "Epoch 47/50\n",
            "25000/25000 [==============================] - 5s 215us/step - loss: 0.0782 - acc: 0.9734 - val_loss: 0.4644 - val_acc: 0.8942\n",
            "Epoch 48/50\n",
            "25000/25000 [==============================] - 5s 215us/step - loss: 0.0763 - acc: 0.9742 - val_loss: 0.4457 - val_acc: 0.8942\n",
            "Epoch 49/50\n",
            "25000/25000 [==============================] - 5s 214us/step - loss: 0.0803 - acc: 0.9724 - val_loss: 0.4445 - val_acc: 0.8988\n",
            "Epoch 50/50\n",
            "25000/25000 [==============================] - 5s 216us/step - loss: 0.0817 - acc: 0.9721 - val_loss: 0.4232 - val_acc: 0.9004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f701e7c9e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlomPF1N2N29",
        "colab_type": "code",
        "outputId": "0674639c-1795-43be-8e93-cc39157e2648",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.evaluate(x_test_gt5,y_test_gt5)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000/5000 [==============================] - 0s 98us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4231841671898961, 0.9004]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FU-HwvIdH0M-"
      },
      "source": [
        "## Sentiment analysis <br> \n",
        "\n",
        "The objective of the second problem is to perform Sentiment analysis from the tweets data collected from the users targeted at various mobile devices.\n",
        "Based on the tweet posted by a user (text), we will classify if the sentiment of the user targeted at a particular mobile device is positive or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nAQDiZHRH0M_"
      },
      "source": [
        "### 6. Read the dataset (tweets.csv) and drop the NA's while reading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3eXGIe-SH0NA",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('./tweets.csv', encoding = \"ISO-8859-1\").dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CWeWe1eJH0NF",
        "outputId": "702bf5eb-74bc-437c-db83-2492d252ccd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3291, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7kX-WoJDH0NV",
        "outputId": "9d3f9155-735d-491a-85e5-0bf1a10c7067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>emotion_in_tweet_is_directed_at</th>\n",
              "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
              "      <td>iPhone</td>\n",
              "      <td>Negative emotion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
              "      <td>iPad or iPhone App</td>\n",
              "      <td>Positive emotion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
              "      <td>iPad</td>\n",
              "      <td>Positive emotion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
              "      <td>iPad or iPhone App</td>\n",
              "      <td>Negative emotion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
              "      <td>Google</td>\n",
              "      <td>Positive emotion</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          tweet_text  ... is_there_an_emotion_directed_at_a_brand_or_product\n",
              "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...  ...                                   Negative emotion\n",
              "1  @jessedee Know about @fludapp ? Awesome iPad/i...  ...                                   Positive emotion\n",
              "2  @swonderlin Can not wait for #iPad 2 also. The...  ...                                   Positive emotion\n",
              "3  @sxsw I hope this year's festival isn't as cra...  ...                                   Negative emotion\n",
              "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...  ...                                   Positive emotion\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OGWB3P2WH0NY"
      },
      "source": [
        "### Consider only rows having Positive emotion and Negative emotion and remove other rows from the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bdgA_8N2H0NY",
        "colab": {}
      },
      "source": [
        "data = data[(data['is_there_an_emotion_directed_at_a_brand_or_product'] == 'Positive emotion') | (data['is_there_an_emotion_directed_at_a_brand_or_product'] == 'Negative emotion')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_Jlu-reIH0Na",
        "outputId": "ff10aec8-29d8-46b1-8cab-8479b8969fe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3191, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J8ibV6EndIE",
        "colab_type": "code",
        "outputId": "9e5876de-3dd3-41b0-b06d-57017aee6f15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "data.head()\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>emotion_in_tweet_is_directed_at</th>\n",
              "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
              "      <td>iPhone</td>\n",
              "      <td>Negative emotion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
              "      <td>iPad or iPhone App</td>\n",
              "      <td>Positive emotion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
              "      <td>iPad</td>\n",
              "      <td>Positive emotion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
              "      <td>iPad or iPhone App</td>\n",
              "      <td>Negative emotion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
              "      <td>Google</td>\n",
              "      <td>Positive emotion</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          tweet_text  ... is_there_an_emotion_directed_at_a_brand_or_product\n",
              "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...  ...                                   Negative emotion\n",
              "1  @jessedee Know about @fludapp ? Awesome iPad/i...  ...                                   Positive emotion\n",
              "2  @swonderlin Can not wait for #iPad 2 also. The...  ...                                   Positive emotion\n",
              "3  @sxsw I hope this year's festival isn't as cra...  ...                                   Negative emotion\n",
              "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...  ...                                   Positive emotion\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SotCRvkDH0Nf"
      },
      "source": [
        "### 7. Represent text as numerical data using `CountVectorizer` and get the document term frequency matrix\n",
        "\n",
        "#### Use `vect` as the variable name for initialising CountVectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YcbkY4sgH0Ng",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vect=CountVectorizer()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi1t1LTTnHJG",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KyXtZGr-H0Nl",
        "colab": {}
      },
      "source": [
        "tweet_text=vect.fit(data['tweet_text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5pxd5fSHH0Nt"
      },
      "source": [
        "### 8. Find number of different words in vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p1DQ2LdNH0Nu",
        "outputId": "40bab078-281d-45c6-8546-e2d4790ae731",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "l=len(tweet_text.get_feature_names())\n",
        "print(\"total no of different words\",l)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total no of different words 5648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dwtgjTBeH0Ny"
      },
      "source": [
        "#### Tip: To see all available functions for an Object use dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ShA6D8jKH0N5"
      },
      "source": [
        "### Find out how many Positive and Negative emotions are there.\n",
        "\n",
        "Hint: Use value_counts on that column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q7LAl5pzH0N6",
        "outputId": "59e3e01f-67eb-45d1-a58f-9d4824faaf63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "pd.value_counts(data['is_there_an_emotion_directed_at_a_brand_or_product'])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Positive emotion    2672\n",
              "Negative emotion     519\n",
              "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IUvgj0FoH0N9"
      },
      "source": [
        "###  Change the labels for Positive and Negative emotions as 1 and 0 respectively and store in a different column in the same dataframe named 'label'\n",
        "\n",
        "Hint: use map on that column and give labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YftKwFv7H0N9",
        "colab": {}
      },
      "source": [
        "data['label'] = data.is_there_an_emotion_directed_at_a_brand_or_product.map({'Positive emotion':1, 'Negative emotion':0})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3YErwYLCH0N_"
      },
      "source": [
        "### 9. Define the feature set (independent variable or X) to be `text` column and `labels` as target (or dependent variable)  and divide into train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lNkwrGgEH0OA",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(data['tweet_text'],data['label'],test_size=0.20,random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q5nlCuaaH0OD"
      },
      "source": [
        "## 10. **Predicting the sentiment:**\n",
        "\n",
        "\n",
        "### Use Naive Bayes and Logistic Regression and their accuracy scores for predicting the sentiment of the given text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2AbVYssaH0OE",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sw-0B33tH0Ox"
      },
      "source": [
        "## 11. Create a function called `tokenize_predict` which can take count vectorizer object as input and prints the accuracy for x (text) and y (labels)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "okCTOs1TH0Oy",
        "colab": {}
      },
      "source": [
        "def tokenize_test(vect):\n",
        "    x_train_dtm = vect.fit_transform(x_train)\n",
        "    print('Features: ', x_train_dtm.shape[1])\n",
        "    x_test_dtm = vect.transform(x_test)\n",
        "    nb = MultinomialNB()\n",
        "    lg=LogisticRegression(solver='lbfgs')\n",
        "    nb.fit(x_train_dtm, y_train)\n",
        "    lg.fit(x_train_dtm, y_train)\n",
        "    y_pred_class_nb = nb.predict(x_test_dtm)\n",
        "    y_pred_class_lg = lg.predict(x_test_dtm)\n",
        "    print('Accuracy when use Navie bayes ', metrics.accuracy_score(y_test, y_pred_class_nb))\n",
        "    print('Accuracy when use Logistic regression ', metrics.accuracy_score(y_test, y_pred_class_lg))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JxZ8jfPEH0O0"
      },
      "source": [
        "### Create a count vectorizer function which includes n_grams = 1,2  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kdCyAN_IH0O0",
        "outputId": "f1eff150-f5fc-4d32-8209-f955a3596e5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# include 1-grams and 2-grams\n",
        "vect = CountVectorizer(ngram_range=(1, 2))\n",
        "tokenize_test(vect)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  25815\n",
            "Accuracy when use Navie bayes  0.838810641627543\n",
            "Accuracy when use Logistic regression  0.8544600938967136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "axepytmgH0O4"
      },
      "source": [
        "### 12. Create a count vectorizer function with stopwords = 'english'  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HToGkq7vH0O4",
        "outputId": "cca13089-1d94-4bf6-8d98-8b21050409a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "vect=CountVectorizer(stop_words='english')\n",
        "tokenize_test(vect)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  4796\n",
            "Accuracy when use Navie bayes  0.838810641627543\n",
            "Accuracy when use Logistic regression  0.8513302034428795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iOIlJRxoH0O7"
      },
      "source": [
        "### 13. Create a count vectorizer function with stopwords = 'english' and max_features =300  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6fUhff-oH0O8",
        "outputId": "1a3051e9-8be4-4e64-b73d-67e16e3bc29b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "vect=CountVectorizer(stop_words='english',max_features=300)\n",
        "tokenize_test(vect)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  300\n",
            "Accuracy when use Navie bayes  0.7996870109546166\n",
            "Accuracy when use Logistic regression  0.8231611893583725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S2KZNWVkH0PA"
      },
      "source": [
        "### 14. Create a count vectorizer function with n_grams = 1,2  and max_features = 15000  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3v9XD082H0PB",
        "outputId": "79722234-3565-4f64-b8ef-964637a26bb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "vect=CountVectorizer(ngram_range=(1, 2),max_features=15000)\n",
        "tokenize_test(vect)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  15000\n",
            "Accuracy when use Navie bayes  0.8341158059467919\n",
            "Accuracy when use Logistic regression  0.8622848200312989\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "We3JK_SRH0PO"
      },
      "source": [
        "### 15. Create a count vectorizer function with n_grams = 1,2  and include terms that appear at least 2 times (min_df = 2)  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fUHrfDCyH0PP",
        "outputId": "b55e8f54-0fd8-4766-a0a9-592050165a98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "vect=CountVectorizer(ngram_range=(1, 2),min_df=2)\n",
        "tokenize_test(vect)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  8298\n",
            "Accuracy when use Navie bayes  0.8435054773082942\n",
            "Accuracy when use Logistic regression  0.8622848200312989\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gnasih44IQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}